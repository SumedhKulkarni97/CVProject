# Project: "EagleEye" - An Advanced Driver-Assistance (ADAS) Perception Pipeline

## 1. Project Objective

The "EagleEye" project is an advanced computer vision pipeline built in Python with OpenCV. 
Its primary goal is to process a single image or a video feed from a "dash cam" perspective and identify all major components of a driving scene.

This system integrates multiple classical and modern computer vision techniques to create a comprehensive understanding of the environment, including:
* Identifying drivable areas (road).
* Detecting lane markings.
* Locating and classifying objects (vehicles, pedestrians).
* Extracting fine-grained features for potential tracking.

## 2. Technical Scope & Features

This project is an end-to-end pipeline that chains the following techniques, fulfilling the "all things" requirement:

1.  Image Pre-processing: Use non-linear operators (morphological transformations) to reduce noise and enhance key features in the raw input frame.
2.  Lane Detection: Use edge detection (Canny) to identify lane markings on the road.
3.  Scene Understanding: Use semantic segmentation to classify every pixel in the image into categories like 'Road', 'Sidewalk', 'Vehicle', or 'Sky'.
4.  Object Detection: Use a pre-trained deep learning model (e.g., YOLOv8) to draw bounding boxes around individual object instances (cars, trucks, pedestrians).
5.  Feature Extraction: For each detected object, use corner detection (Shi-Tomasi) to find stable feature points for tracking.
6.  Advanced Feature Extraction: (Optional) Apply image decomposition (e.g., Wavelet Transform) to an object's patch to create a feature vector.
7.  Object Classification: Use the extracted features to run a simple classifier to refine the object's class (e.g., differentiate 'Car' from 'Truck').

## 3. Tech Stack

* Python 3.10+
* OpenCV (`opencv-python`): For all core CV operations (Canny, morphology, feature detection, etc).
* NumPy: For numerical operations and image array manipulation.
* PyWavelets (`pywt`): (Optional) For the image decomposition step.
* A Pre-trained Model:
    * Object Detection: YOLOv8 (recommended)
    * Semantic Segmentation: DeepLabV3 or a U-Net trained on a driving dataset (e.g., Cityscapes).

## 4. The "EagleEye" Pipeline

The project operates as a multi-stage data-flow. Each step processes the output of the previous one.



1.  Input: Read a frame from a video file or camera.
2.  Clean: Apply a Gaussian Blur and morphological 'Opening' to the frame to reduce noise. (Non-linear Operator)
3.  Segment: Feed the clean frame into the Semantic Segmentation model. This produces a "mask" image showing the drivable road area. (Semantic Segmentation)
4.  Find Lanes: Create a Region of Interest (ROI) from the "road" mask. Apply Canny (Edge Detection) inside this ROI to find lane lines.
5.  Find Objects: Feed the original clean frame into the YOLO object detector. This produces a list of bounding boxes `[x, y, w, h]` for all cars, pedestrians, etc. (Object Detection)
6.  Analyze Objects (Loop): For each bounding box found:
    a.  Isolate: Crop the main frame to this bounding box (this is the object's patch).
    b.  Find Corners: Run the Shi-Tomasi (Corner Detection) algorithm on the patch to find keypoints.
    c.  Decompose: (Optional) Apply a wavelet transform (Image Decomposition) to the patch to get a feature hash.
    d.  Identify: Feed the patch (or its decomposed features) into a classifier to get a detailed label (e.g., 'SUV', 'Sedan'). (Classification)
7.  Visualize: Draw the segmentation mask, lane lines, bounding boxes, and keypoints all onto the original frame.
8.  Output: Display the final, annotated frame.

## 5. Datasets

For testing and training:
KITTI Dataset : The gold standard for ADAS research.

1. KITTI-Lite (Shuffled Subset)
Purpose: Optimized for training and testing static object detection models (YOLO, SSD).
Structure: A "flat" collection of independent images (e.g., 000000.png, 000001.png).
Temporal Logic: None. These images are typically sampled from different drives, locations, and times.
Best Use Case:  Training YOLO to recognize "Car" vs. "Pedestrian" in diverse environments.
Testing Wavelet-based feature extraction on various textures.
Limitation: It is impossible to run Optical Flow or Velocity Estimation because there is no continuity between Frame A and Frame B.

2. KITTI Raw (Sequential/Synced Data)
Purpose: Designed for temporal analysis, motion tracking, and autonomous navigation.
Structure: Images are organized into "Drives" (e.g., 2011_09_26_drive_0020_sync) and sub-cameras (e.g., image_02).
Temporal Logic: High. Images are recorded at a constant frequency (approx. 10 frames per second), meaning each frame is strictly 0.1 seconds apart.
Best Use Case:
    Optical Flow: Calculating how pixels shift between t and t+1.
    Velocity Estimation: Using the timestamps.txt file to calculate real-world speed (km/h).